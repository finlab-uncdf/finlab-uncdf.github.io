<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	
	<meta http-equiv="x-ua-compatible" content="ie=edge">
	
	<title>UNCDF Toolkit | A-B-n Testing</title>
	
	<meta name="description" content="">
	
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	
	<link rel="stylesheet" href="../css/toolpurple.css">
</head>
<body>
	<div class="header-section">
		<div class="header-container">
			<h1><a href="../index.html">UNCDF Toolkit</a></h1>
			
						<div class="header-links">
				<div class="header-link"><a href="../index.html">
					<button class="header-button">Back</button>
				</a></div>
			</div>
		</div>
	</div>
	
	<div class="tool-section">
		<div class="tool-container">
			<div class="tool-grid">
				<div class="tool-name">
					<div class="tool-name-text">
						<h3><span>HUMAN CENTERED DESIGN</span> | TESTING</h3>
						
						<h1>A-B-n Testing</h1>
						
						<p class="time">60-90 Min (Actual time for testing extra)</p>
					</div>
					
					<div class="tool-about">
						<h2>About</h2>
						
						<p>A/B testing, at its most basic, is an experiment to compare two versions of a prototype (product or service) and figure which performs better. A/B/n testing involves testing at least three variations of something. The “n” refers to the number of variations that can be tested. While the term is generally used in the context of web development, the approach is  relevant to other testing contexts too. Testing ideally is done with sample sizes that are statistically significant, but can also be done effectively  with users that can provide good quality feedback to be collected.
</p>
					</div>
					
					<div class="tool-use-case">
						<h2>Use Cases</h2>
						
						<ul>
<li>A/B/n Testing can be used to test early low-fidelity concepts and prototypes, or evolved high fidelity experienceable prototypes. </li>
<li>Typically A/B/n testing is used to test two or more variations of a product or service at a broad level. Especially at the concept and prototype level</li>
<li>Or test two or more variations of specific components, journeys, features etc. for a product or service - closer to production and launch. </li>
</ul>
					</div>
					
					<div class="tool-limitations">
						<h2>Limitations</h2>
						
						<p>It may not be feasible to comprehensively test each aspect of the design. The decision on what is to be tested and why lies with the designer. The method also does not itself specify how variations must differ from each other. </p>
					</div>
				</div>
				
				<div class="tool-image-illustration">
					<img src="../illustrations/A-B-n Testing_illust.png" alt="" />
                    <a class="linkin "href=" https://www.youtube.com/watch?v=R1jAoOvWRN0 " target="_blank">User Testing | PlaybookUX</a>

				</div>
			</div>
		</div>
	</div>
	
	<div class="under-section">
		<div class="under-container">
			<div class="under-grid">
				<div class="tool-under-image">
<img src="../illustrations/A-B-n Testing_toolcard.png" alt="" />
</div>
				
				<div class="tool-understand">
					<h2>Understand</h2>
					
					<ul>
<li>The ‘<span>Purpose of Test</span>’ section is meant for you to note down the key reason for which the test is being conducted - to understand broad preference, to seek adoption, to test a new version etc. </li>
<li>The ‘<span>Key Metrics of Success/Failure</span>’ box is meant to note down the key metrics based on which test results will be evaluated on - interest, time spent on page, signup, purchase, revenues, rating score etc. </li>
<li>‘<span>Testing Formats</span>’ is the section in which the key format of the test assets and experiment have to be described - visual concepts and prototypes, interactive screens, in production assets etc. The test may involve actual use that can be recorded, or it may just involve an interview where feedback is collected.  </li>
<li>‘<span>Testing Scope</span>’ is the section in which the designer has to define how each of the versions are different from each other - along with rationale on why the differences are relevant. This would include specifying constants across the versions, as well as variables in each version. Based on one round of testing, iterations can be developed and tested again. </li>
<li>‘<span>Tester Profile</span>’ is the section in which the designer needs to identify who the different versions are to be tested with. Usually, in A/B testing there is a ‘<span>Control</span>’ group that a base version is tested with, and ‘<span>Treatment</span>’ groups with which different versions are tested. The  different types (for example - existing customers, loyal customers, not customers) of users have to be defined based on the purpose, format and scope of the test. Tests are typically considered  valid if different groups of a single  user type test all versions (for example - existing customers groups 1,2,n  testing version A, B, n). Tests can also be valid if different versions are tested with one group of one type (for example - group 1 of existing customers test version A,B,n). Tests are invalid if different versions are tested with different types of users ( for example - A with existing, B with loyal, C with not customers) as relevant comparisons cannot be made. </li>
<li>‘<span>Test Learnings</span>’ is the section in which one is to note down the results of testing - in terms of results (in terms of metrics) achieved or just a collation of the feedback received from testers. </li>
</ul>
				</div>
				
				
				<div class="tool-steps">
					<div class="tool-step-grid">
						<div class="tool-step-segment-title">
							<h2>Step by Step</h2>
						</div>
						
						<ol>
<li><span>Familiarise yourself:</span> Read through the template and discuss the testing requirements. Note down the purpose of the test and key metrics. </li>
<li><span>Versions, Format & Scope:</span> Note down the format that the test would take and the kind of assets that would need to be developed (if not done already). Make a note of how each of the versions differ from each other. </li>
<li><span>Identify Tester Profile:</span> Note down the kind of testers and users the testing will be focused on. Profiles, segments, groups etc. can be mentioned here along with the sample size.</li>
<li><span>Conduct the Test:</span> Use the tool as guidance as you go about the test. The test will require an additional script (steps, tasks etc) that will depend on your choice of format, scope, and profile. </li>
<li><span>Test Learnings & Review:</span> Once the tests are done - note down the results and learnings in this section. If a version meets the purpose - move ahead. Otherwise, discuss iteration of versions and further testing needs. </li>
</ol>
					</div>
				</div>
				
				
			</div>
		</div>
	</div>
    <div class="f-section">
		<div class="f-container">
			<div class="f-grid">
				<div class="f-intro-text">
					<h2>Facilitators Section</h2>
				</div>
				
				<div class="f-how-to">
					<h2>How to for Facilitators</h2>
					
					<ol>
<li><span>At the start:</span> Make sure the participants understand the goal of the activity and the directions (refer to facilitation questions if they are feeling stuck).</li>
<li><span>During the exercise:</span> Start with a discussion on testing approaches that each of the teams have used before. Walk through the tool and its components. Discuss why testing is required and what is the most crucial aspect to test. Connect back to the prototypes that people may have created already. </li>
<li><span>When summarising:</span> Have participants walk you through the worksheet they have filled out. Probe them on their final testing plans, versions and approaches</li>
</ol>
				</div>
				
				<div class="f-question-bank">
					<h2>Facilitators Question Bank</h2>
					
					<ul>
<li>What are some testing approaches you may have used before? Does anyone know what A/B/n testing is?</li>
<li>What is the purpose of the testing you want to do? Do your prototypes reflect that?</li>
<li>How will you measure testing results?  What metrics are key?</li>
<li>What is the format of testing? Are you asking respondents to interact or use something? Or are you presenting solutions and asking them to provide feedback?</li>
<li>Do you have a single prototype or have you build the variations already?</li>
<li>How do your versions differ from each other? What remains constant? What are some variables? </li>
<li>Who are you going to test these variations with? Do you have clarity on the different types and personas of customers? Are you going to test with all or just a specific type? </li>
<li>Are you going to take all versions to some users? Who are these and why do you think that is useful?</li>
<li>How much time will you need to get testing done? Is that time practical?</li>
<li>How will you record results? </li>
</ul>
				</div>
			</div>
		</div>
	</div>
	<div class="down-section">
		<div class="down-container">
			<div class="down-grid">
				<div class="tool-card-image">
											<img src="../illustrations/tool-image.png" alt="" />
											
											<div class="download-link"><a href="../tools/images/A-B-n Testing.png" download=" A-B-n Testing_toolcard">
												<button class="download-button">Download Tool!</button>
											</a></div>
											
										</div>
										
										<div class="f-deck-image">
											<img src="../illustrations/f-deck-image.png" alt="" />
											
											<div class="download-link"><a href="/#" download=" A-B-n Testing_facilitation">
													<button class="download-button">Download Facilitation Slides!</button>
												</a></div>
											
										</div>
				
				
			</div>
		</div>
	</div>
	
</body>
</html>